{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d05974e8",
      "metadata": {
        "id": "d05974e8"
      },
      "source": [
        "# Vision Transformer (ViT) Architecture Overview\n",
        "\n",
        "Vision Transformer architecture comprises several key stages:\n",
        "\n",
        "* Image Patching and Embedding\n",
        "* Positional Encoding\n",
        "* Transformer Encoder\n",
        "* Classification Head (MLP Head)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dafad8a0",
      "metadata": {
        "id": "dafad8a0"
      },
      "source": [
        "![ViT Diagram](https://media.geeksforgeeks.org/wp-content/uploads/20250108160202257232/Vision-Transformer-Architecture_.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5861f23f",
      "metadata": {
        "id": "5861f23f"
      },
      "source": [
        "\n",
        "## Table of Contents\n",
        "- [1. Image Patching and Embedding](#1-image-patching-and-embedding)\n",
        "- [2. Positional Encoding](#2-positional-encoding)\n",
        "- [3. Transformer Encoder Layers](#3-transformer-encoder-layers)\n",
        "  - [Multi-Head Self-Attention (MSA)](#1-multi-head-self-attention-msa)\n",
        "  - [Feed-Forward Network (FFN)](#2-feed-forward-network-ffn)\n",
        "  - [Stacking Encoder Layers](#stacking-encoder-layers)\n",
        "- [4. Classification Token (CLS Token)](#4-classification-token-cls-token)\n",
        "- [5. MLP Head (Classification Head)](#5-mlp-head-classification-head)\n",
        "- [Vision Transformer Architecture Summary](#vision-transformer-architecture-summary)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd8cc6ae",
      "metadata": {
        "id": "bd8cc6ae"
      },
      "source": [
        "\n",
        "\n",
        "## 1. Image Patching and Embedding\n",
        "\n",
        "The first and most critical step in the ViT pipeline is to convert the image into a sequence of patches, similar to the tokens in an NLP model.\n",
        "\n",
        "* **Patch Splitting:** The input image, usually of size H×W×C (height, width, and channels), is divided into fixed-size patches. For example, an image of size 224x224 can be split into non-overlapping 16x16 patches, resulting in 224/16 × 224/16 = 14×14 = 196 patches.\n",
        "\n",
        "* **Patch Flattening:** Each patch is then flattened into a 1D vector. A patch of size P×P×C (e.g., 16x16x3) is reshaped into a vector of size P²×C, creating 196 patch vectors for an image.\n",
        "\n",
        "* **Patch Embedding:** Each flattened patch is projected into a higher-dimensional space (embedding dimension D) through a learnable linear projection. This linear transformation enables the model to learn richer feature representations for each patch. The result is a sequence of patch embeddings, each representing a part of the image. The total number of patches in the sequence is N = H/P × W/P, where N is the number of patches. For instance, with a 224x224 image and 16x16 patches, we have 196 patches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0bbf9eb4",
      "metadata": {
        "id": "0bbf9eb4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2) # flatten last two dimensions and transpose from (B, embding_dim, num_patches) to (B, num_patches, embding_dim)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db9c94f9",
      "metadata": {
        "id": "db9c94f9"
      },
      "source": [
        "\n",
        "## 2. Positional Encoding\n",
        "\n",
        "Transformers do not inherently capture the spatial order of input sequences. Since the patches are processed as independent tokens, it's essential to introduce **positional encodings** to retain the spatial structure of the original image.\n",
        "\n",
        "* **Positional Embedding:** Positional encodings are added to each patch embedding to encode information about the location of patches within the image. These embeddings help the model understand the spatial relationships between patches, similar to how transformers in NLP encode the positions of words in a sentence.\n",
        "\n",
        "* **Learned vs. Fixed Positional Encoding:** In ViTs, positional encodings can either be learned during training or predefined (fixed). Most implementations of Vision Transformers use learnable positional encodings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b79f8fce",
      "metadata": {
        "id": "b79f8fce"
      },
      "outputs": [],
      "source": [
        "# 2. Adding Positional Embeddings\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, seq_len):\n",
        "        super().__init__()\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, seq_len + 1, embed_dim))  #1 is batch size, adjusted for [CLS] token, embed_dim is embedding dimension\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pos_embed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "775b51f6",
      "metadata": {
        "id": "775b51f6"
      },
      "source": [
        "## 3. Transformer Encoder Layers\n",
        "\n",
        "Once the patches are embedded and augmented with positional information, they are passed through a stack of transformer encoder layers. These layers consist of two primary components: Multi-Head Self-Attention (MSA) and a Feed-Forward Neural Network (FFN).\n",
        "\n",
        "### 1. Multi-Head Self-Attention (MSA)\n",
        "\n",
        "* **Self-Attention:** The self-attention mechanism allows each patch to attend to every other patch in the sequence. This means that the transformer can model long-range dependencies and relationships between different parts of the image. Each patch computes a weighted sum of the values of all other patches based on its similarity to them, known as the attention score.\n",
        "\n",
        "  Attention(Q,K,V) = softmax(QK^T / √d_k)V\n",
        "\n",
        "  Where Q (query), K (key), and V (value) are learned linear projections of the input patch embeddings.\n",
        "  - The dot product between queries and keys determines the attention score, and softmax normalizes it.\n",
        "  - The weighted sum of values determines the output.\n",
        "\n",
        "* **Multi-Head Attention:** The attention mechanism is computed in parallel across multiple attention heads, allowing the model to focus on different parts of the image simultaneously.\n",
        "\n",
        "### 2. Feed-Forward Network (FFN)\n",
        "\n",
        "After self-attention, the patches are passed through a **feed-forward network (FFN)**. The FFN consists of two fully connected layers with a non-linear activation function (typically GELU) in between.\n",
        "\n",
        "Each transformer encoder layer includes residual (skip) connections and layer normalization to stabilize training and improve convergence. These techniques ensure that the deeper layers do not lose important information from the earlier layers.\n",
        "\n",
        "### Stacking Encoder Layers\n",
        "\n",
        "Multiple transformer encoder layers (e.g., 12, 24 layers) are stacked on top of each other. Each layer refines the patch embeddings, allowing the model to build more complex and abstract representations of the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d263ab9c",
      "metadata": {
        "id": "d263ab9c"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.attn(x, x, x)[0] # return only the output, ignore the attention weights and 3 x use for query, key, value"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e061fae0",
      "metadata": {
        "id": "e061fae0"
      },
      "source": [
        "## 4. Classification Token (CLS Token)\n",
        "\n",
        "In Vision Transformers, a special classification token (CLS token) is introduced at the beginning of the input sequence. This token serves a critical role: it gathers information from all the patches throughout the transformer layers.\n",
        "\n",
        "The CLS token learns to represent the entire image by attending to the different patches through the self-attention mechanism. At the output of the transformer layers, the CLS token is extracted and passed to a classifier for the final prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "320c498a",
      "metadata": {
        "id": "320c498a"
      },
      "source": [
        "## 5. MLP Head (Classification Head)\n",
        "\n",
        "After the transformer encoders process the sequence of patches and the CLS token, the output corresponding to the CLS token is used for classification.\n",
        "\n",
        "The output of the CLS token is fed into an MLP, typically consisting of one or two fully connected layers. A softmax layer is applied at the end of the MLP for classification tasks, predicting the image's label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4fa60a80",
      "metadata": {
        "id": "4fa60a80"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(embed_dim, num_heads)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(mlp_dim, embed_dim)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x)) # Residual connection as paper described\n",
        "        x = x + self.mlp(self.norm2(x)) # Residual connection as paper described\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41f42edb",
      "metadata": {
        "id": "41f42edb"
      },
      "source": [
        "## Building the Vision Transformer Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "eddb0543",
      "metadata": {
        "id": "eddb0543"
      },
      "outputs": [],
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, num_classes=10, embed_dim=768, num_heads=8, depth=6, mlp_dim=1024):\n",
        "        super().__init__()\n",
        "        self.patch_embedding = PatchEmbedding(img_size, patch_size, 3, embed_dim)\n",
        "        self.pos_encoding = PositionalEncoding(embed_dim, (img_size // patch_size) ** 2)\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerEncoderBlock(embed_dim, num_heads, mlp_dim) for _ in range(depth)\n",
        "        ])\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "        self.mlp_head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        x = self.patch_embedding(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = self.pos_encoding(x)\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x)\n",
        "        return self.mlp_head(x[:, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training the Vision Transformer"
      ],
      "metadata": {
        "id": "Zgd-tSoBf7YS"
      },
      "id": "Zgd-tSoBf7YS"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Device configuration (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Dataset and DataLoader\n",
        "train_data = datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_data,\n",
        "    batch_size=32,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Model\n",
        "model = VisionTransformer().to(device)\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        # Move tensors to device (GPU/CPU)\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/5], Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG0B5NslgAbr",
        "outputId": "9b79aca1-6102-45ad-f5a5-98ea59fc9990"
      },
      "id": "QG0B5NslgAbr",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 43.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 2.8157\n",
            "Epoch [2/5], Loss: 2.3300\n",
            "Epoch [3/5], Loss: 2.3229\n",
            "Epoch [4/5], Loss: 2.3198\n",
            "Epoch [5/5], Loss: 2.3186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2425096",
      "metadata": {
        "id": "e2425096"
      },
      "source": [
        "## Vision Transformer Architecture Summary\n",
        "\n",
        "To summarize, the Vision Transformer architecture involves the following key steps:\n",
        "\n",
        "1. **Input Image Processing:** The input image is divided into patches, which are flattened and embedded using a linear projection.\n",
        "\n",
        "2. **Positional Encoding:** Positional encodings are added to the patch embeddings to retain spatial information.\n",
        "\n",
        "3. **Transformer Encoder:** The patch embeddings (along with the CLS token) are passed through multiple transformer encoder layers, which include multi-head self-attention and feed-forward networks.\n",
        "\n",
        "4. **Classification Head:** The CLS token's output is extracted and fed into an MLP for final classification."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##pretrained model"
      ],
      "metadata": {
        "id": "tsiB_LE1o80L"
      },
      "id": "tsiB_LE1o80L"
    },
    {
      "cell_type": "code",
      "source": [
        "## Import Libraries\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import timm\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "VgjimmlVvKOH"
      },
      "id": "VgjimmlVvKOH",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load CIFAR-10 Dataset\n",
        "# Define transforms for CIFAR-10 (32x32) to ViT input size (224x224)\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize(224),  # Resize to ViT input size\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                       std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "metadata": {
        "id": "B7ZMauWjvPf5"
      },
      "id": "B7ZMauWjvPf5",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                       std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "aybY4GXJvaAp"
      },
      "id": "aybY4GXJvaAp",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CIFAR-10 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform_train\n",
        ")"
      ],
      "metadata": {
        "id": "MXdwQR8evdu0"
      },
      "id": "MXdwQR8evdu0",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform_test\n",
        ")"
      ],
      "metadata": {
        "id": "3l1A25IIvjXG"
      },
      "id": "3l1A25IIvjXG",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxAcsPuXvnSX",
        "outputId": "cd0e5ba4-a322-4dc5-887a-39c68e26db6d"
      },
      "id": "NxAcsPuXvnSX",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 50000\n",
            "Test samples: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained ViT model from timm library\n",
        "model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=10)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"Model: {model.__class__.__name__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "4cba6b3179684e92b1f335724e9bd444",
            "7dc45d9fc17e4d3785fd4a29921e166b",
            "5b6bec4a2d5044f8a3c3f31cb2a102da",
            "b82b803bb0ec49b69e04e3931ea5eb01",
            "a087040bb7ae45e8a3ec829114e022c8",
            "e2efcae8a9e54d45974ac1cb40595ea8",
            "8d36ffafd66244e18d4759750727abf5",
            "f2e60adae77e4d6da52fd07f79e3d6e2",
            "8cc2aa9c39c3411cb2e00df3ab1a3b65",
            "93796682241b4e53946f48e0d3002f0d",
            "cd8e4ffce9304a7aba7f3c18b0dab5a8"
          ]
        },
        "id": "Pdk436GkvpPV",
        "outputId": "aedf26f2-da9b-4133-83f0-513c8ee4abac"
      },
      "id": "Pdk436GkvpPV",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4cba6b3179684e92b1f335724e9bd444"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Model: VisionTransformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Define Loss Function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
      ],
      "metadata": {
        "id": "yKdxsTeRvyt4"
      },
      "id": "yKdxsTeRvyt4",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Training Function\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc"
      ],
      "metadata": {
        "id": "NKXxLw2fv69q"
      },
      "id": "NKXxLw2fv69q",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Evaluation Function\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(test_loader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc"
      ],
      "metadata": {
        "id": "L5Tvt0CuwAsk"
      },
      "id": "L5Tvt0CuwAsk",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Train the Model\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "    # Evaluate\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_rGaiADwFv-",
        "outputId": "fbd3ad4e-386f-4c50-f12a-07976fc804da"
      },
      "id": "R_rGaiADwFv-",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 1563/1563 [27:41<00:00,  1.06s/it]\n",
            "Evaluating: 100%|██████████| 313/313 [01:53<00:00,  2.77it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.2382, Train Acc: 92.28%\n",
            "Test Loss: 0.2292, Test Acc: 92.56%\n",
            "\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 1563/1563 [27:37<00:00,  1.06s/it]\n",
            "Evaluating: 100%|██████████| 313/313 [01:53<00:00,  2.77it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.1591, Train Acc: 94.72%\n",
            "Test Loss: 0.1849, Test Acc: 93.71%\n",
            "\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 1563/1563 [27:36<00:00,  1.06s/it]\n",
            "Evaluating: 100%|██████████| 313/313 [01:53<00:00,  2.76it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.1301, Train Acc: 95.57%\n",
            "Test Loss: 0.1861, Test Acc: 94.03%\n",
            "\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 1563/1563 [27:36<00:00,  1.06s/it]\n",
            "Evaluating: 100%|██████████| 313/313 [01:53<00:00,  2.76it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.1094, Train Acc: 96.26%\n",
            "Test Loss: 0.2021, Test Acc: 93.71%\n",
            "\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 1563/1563 [27:38<00:00,  1.06s/it]\n",
            "Evaluating: 100%|██████████| 313/313 [01:53<00:00,  2.76it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0963, Train Acc: 96.67%\n",
            "Test Loss: 0.1961, Test Acc: 93.81%\n",
            "\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 1563/1563 [27:36<00:00,  1.06s/it]\n",
            "Evaluating: 100%|██████████| 313/313 [01:53<00:00,  2.76it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0180, Train Acc: 99.38%\n",
            "Test Loss: 0.1118, Test Acc: 96.76%\n",
            "\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1563/1563 [27:37<00:00,  1.06s/it]\n",
            "Evaluating: 100%|██████████| 313/313 [01:53<00:00,  2.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0032, Train Acc: 99.91%\n",
            "Test Loss: 0.1340, Test Acc: 96.64%\n",
            "\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1563/1563 [27:36<00:00,  1.06s/it]\n",
            "Evaluating: 100%|██████████| 313/313 [01:53<00:00,  2.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0019, Train Acc: 99.96%\n",
            "Test Loss: 0.1532, Test Acc: 96.77%\n",
            "\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  44%|████▍     | 690/1563 [12:11<15:25,  1.06s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'vit_cifar10.pth')\n",
        "print(\"Model saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "Bsc7xbB-wTAd",
        "outputId": "5ea8cb85-733e-481b-ba7a-62b4cc0e4622"
      },
      "id": "Bsc7xbB-wTAd",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2132040856.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vit_cifar10.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model saved successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "model = timm.create_model('vit_base_patch16_224', pretrained=False, num_classes=10)\n",
        "model.load_state_dict(torch.load('vit_cifar10.pth')) # load the train model\n",
        "model = model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "zYO3lcJ0wcle"
      },
      "id": "zYO3lcJ0wcle",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test on a single image\n",
        "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "metadata": {
        "id": "z-t2QKdFwmbX"
      },
      "id": "z-t2QKdFwmbX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a sample image\n",
        "sample_image, sample_label = test_dataset[0]\n",
        "sample_image = sample_image.unsqueeze(0).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(sample_image)\n",
        "    _, predicted = output.max(1)\n",
        "\n",
        "print(f\"Predicted: {classes[predicted.item()]}\")\n",
        "print(f\"Actual: {classes[sample_label]}\")"
      ],
      "metadata": {
        "id": "IgsP4IZGwp8e"
      },
      "id": "IgsP4IZGwp8e",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4cba6b3179684e92b1f335724e9bd444": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7dc45d9fc17e4d3785fd4a29921e166b",
              "IPY_MODEL_5b6bec4a2d5044f8a3c3f31cb2a102da",
              "IPY_MODEL_b82b803bb0ec49b69e04e3931ea5eb01"
            ],
            "layout": "IPY_MODEL_a087040bb7ae45e8a3ec829114e022c8"
          }
        },
        "7dc45d9fc17e4d3785fd4a29921e166b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2efcae8a9e54d45974ac1cb40595ea8",
            "placeholder": "​",
            "style": "IPY_MODEL_8d36ffafd66244e18d4759750727abf5",
            "value": "model.safetensors: 100%"
          }
        },
        "5b6bec4a2d5044f8a3c3f31cb2a102da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2e60adae77e4d6da52fd07f79e3d6e2",
            "max": 346284714,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8cc2aa9c39c3411cb2e00df3ab1a3b65",
            "value": 346284714
          }
        },
        "b82b803bb0ec49b69e04e3931ea5eb01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93796682241b4e53946f48e0d3002f0d",
            "placeholder": "​",
            "style": "IPY_MODEL_cd8e4ffce9304a7aba7f3c18b0dab5a8",
            "value": " 346M/346M [00:02&lt;00:00, 235MB/s]"
          }
        },
        "a087040bb7ae45e8a3ec829114e022c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2efcae8a9e54d45974ac1cb40595ea8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d36ffafd66244e18d4759750727abf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2e60adae77e4d6da52fd07f79e3d6e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cc2aa9c39c3411cb2e00df3ab1a3b65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "93796682241b4e53946f48e0d3002f0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd8e4ffce9304a7aba7f3c18b0dab5a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}