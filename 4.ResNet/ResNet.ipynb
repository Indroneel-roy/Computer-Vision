{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Import libraries"
      ],
      "metadata": {
        "id": "YlaomTeLiBs3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fjdNpbTuh6DG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\"Bottleneck block for ResNet-50, 101, 152\"\"\"\n",
        "    def __init__(self, in_channels, intermediate_channels, identity_downsample=None, stride=1):\n",
        "        super(Block, self).__init__()\n",
        "        self.expansion = 4\n",
        "\n",
        "        # 1x1 convolution\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels,\n",
        "            intermediate_channels,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "            bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(intermediate_channels)\n",
        "\n",
        "        # 3x3 convolution\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            intermediate_channels,\n",
        "            intermediate_channels,\n",
        "            kernel_size=3,\n",
        "            stride=stride,\n",
        "            padding=1,\n",
        "            bias=False\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(intermediate_channels)\n",
        "\n",
        "        # 1x1 convolution (expansion)\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            intermediate_channels,\n",
        "            intermediate_channels * self.expansion,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "            bias=False\n",
        "        )\n",
        "        self.bn3 = nn.BatchNorm2d(intermediate_channels * self.expansion)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.identity_downsample = identity_downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x.clone()\n",
        "\n",
        "        # First conv block\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # Second conv block\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # Third conv block\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "\n",
        "        # Apply identity downsample if needed\n",
        "        if self.identity_downsample is not None:\n",
        "            identity = self.identity_downsample(identity)\n",
        "\n",
        "        # Add skip connection\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "ByWOVeQNiMsR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, image_channels, num_classes):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "\n",
        "        # Initial convolution\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            image_channels, 64, kernel_size=7, stride=2, padding=3, bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # ResNet layers\n",
        "        self.layer1 = self._make_layer(\n",
        "            block, layers[0], intermediate_channels=64, stride=1\n",
        "        )\n",
        "        self.layer2 = self._make_layer(\n",
        "            block, layers[1], intermediate_channels=128, stride=2\n",
        "        )\n",
        "        self.layer3 = self._make_layer(\n",
        "            block, layers[2], intermediate_channels=256, stride=2\n",
        "        )\n",
        "        self.layer4 = self._make_layer(\n",
        "            block, layers[3], intermediate_channels=512, stride=2\n",
        "        )\n",
        "\n",
        "        # Classification head\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * 4, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial layers\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        # ResNet layers\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        # Classification\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _make_layer(self, block, num_residual_blocks, intermediate_channels, stride):\n",
        "        identity_downsample = None\n",
        "        layers = []\n",
        "\n",
        "        # Check if we need to downsample\n",
        "        if stride != 1 or self.in_channels != intermediate_channels * 4:\n",
        "            identity_downsample = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    self.in_channels,\n",
        "                    intermediate_channels * 4,\n",
        "                    kernel_size=1,\n",
        "                    stride=stride,\n",
        "                    bias=False,\n",
        "                ),\n",
        "                nn.BatchNorm2d(intermediate_channels * 4),\n",
        "            )\n",
        "\n",
        "        # First block (may downsample)\n",
        "        layers.append(\n",
        "            block(self.in_channels, intermediate_channels, identity_downsample, stride)\n",
        "        )\n",
        "\n",
        "        # Update in_channels\n",
        "        self.in_channels = intermediate_channels * 4\n",
        "\n",
        "        # Remaining blocks\n",
        "        for i in range(num_residual_blocks - 1):\n",
        "            layers.append(block(self.in_channels, intermediate_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n"
      ],
      "metadata": {
        "id": "pPMlcOEFkUHJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ResNet50(img_channel=3, num_classes=1000):\n",
        "    return ResNet(Block, [3, 4, 6, 3], img_channel, num_classes)\n",
        "\n",
        "\n",
        "def ResNet101(img_channel=3, num_classes=1000):\n",
        "    return ResNet(Block, [3, 4, 23, 3], img_channel, num_classes)\n",
        "\n",
        "\n",
        "def ResNet152(img_channel=3, num_classes=1000):\n",
        "    return ResNet(Block, [3, 8, 36, 3], img_channel, num_classes)\n"
      ],
      "metadata": {
        "id": "NtPvxcmctfRP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_valid_loader(data_dir, batch_size, augment, random_seed, valid_size=0.1, shuffle=True):\n",
        "    \"\"\"Get training and validation data loaders for CIFAR-10\"\"\"\n",
        "\n",
        "    # CIFAR-10 normalization\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.4914, 0.4822, 0.4465],\n",
        "        std=[0.2023, 0.1994, 0.2010]\n",
        "    )\n",
        "\n",
        "    # Common transforms\n",
        "    common_transform = [\n",
        "        transforms.Resize((224, 224)),  # Resize to standard ResNet input\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ]\n",
        "\n",
        "    # Training transforms with optional augmentation\n",
        "    if augment:\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomCrop(224, padding=4),\n",
        "            transforms.ToTensor(),\n",
        "            normalize\n",
        "        ])\n",
        "    else:\n",
        "        train_transform = transforms.Compose(common_transform)\n",
        "\n",
        "    valid_transform = transforms.Compose(common_transform)\n",
        "\n",
        "    # Load datasets\n",
        "    train_dataset = datasets.CIFAR10(\n",
        "        root=data_dir, train=True, download=True, transform=train_transform\n",
        "    )\n",
        "    valid_dataset = datasets.CIFAR10(\n",
        "        root=data_dir, train=True, download=True, transform=valid_transform\n",
        "    )\n",
        "\n",
        "    # Create train/valid split\n",
        "    num_train = len(train_dataset)\n",
        "    indices = list(range(num_train))\n",
        "    split = int(np.floor(valid_size * num_train))\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.seed(random_seed)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    train_idx, valid_idx = indices[split:], indices[:split]\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=batch_size, sampler=train_sampler\n",
        "    )\n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset, batch_size=batch_size, sampler=valid_sampler\n",
        "    )\n",
        "\n",
        "    return train_loader, valid_loader"
      ],
      "metadata": {
        "id": "4LYkZLOcv2yF"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_test_loader(data_dir, batch_size, shuffle=True):\n",
        "    \"\"\"Get test data loader for CIFAR-10\"\"\"\n",
        "\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.4914, 0.4822, 0.4465],\n",
        "        std=[0.2023, 0.1994, 0.2010]\n",
        "    )\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "\n",
        "    dataset = datasets.CIFAR10(\n",
        "        root=data_dir, train=False, download=True, transform=transform\n",
        "    )\n",
        "\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=shuffle\n",
        "    )\n",
        "\n",
        "    return data_loader"
      ],
      "metadata": {
        "id": "hQLHLlCq05Er"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "data_dir = './data'\n",
        "num_classes = 10  # CIFAR-10 has 10 classes\n",
        "num_epochs = 7\n",
        "batch_size = 64\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load data\n",
        "train_loader, valid_loader = get_train_valid_loader(\n",
        "    data_dir=data_dir,\n",
        "    batch_size=batch_size,\n",
        "    augment=True,  # Enable data augmentation\n",
        "    random_seed=1\n",
        ")\n",
        "\n",
        "test_loader = get_test_loader(data_dir=data_dir, batch_size=batch_size)\n",
        "\n",
        "# Create model - FIXED: Using correct num_classes\n",
        "model = ResNet50(img_channel=3, num_classes=num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=learning_rate,\n",
        "    weight_decay=0.0001,\n",
        "    momentum=0.9\n",
        ")\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQIpB6QIv7ee",
        "outputId": "bb7102c8-4c41-4848-8fbf-e5a27c296343"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training"
      ],
      "metadata": {
        "id": "WQH_ZrhDwzEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "total_step = len(train_loader)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Print progress every 100 steps\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Epoch summary\n",
        "    avg_loss = running_loss / total_step\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}] - Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in valid_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f'Validation Accuracy: {accuracy:.2f}%\\n')\n",
        "\n",
        "    # Step the scheduler\n",
        "    scheduler.step()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYRg_A73wrHM",
        "outputId": "f0b07b5c-d875-4572-c032-1cd98e54ba3c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/7], Step [100/704], Loss: 1.9643\n",
            "Epoch [1/7], Step [200/704], Loss: 1.9841\n",
            "Epoch [1/7], Step [300/704], Loss: 1.5853\n",
            "Epoch [1/7], Step [400/704], Loss: 1.6292\n",
            "Epoch [1/7], Step [500/704], Loss: 1.7593\n",
            "Epoch [1/7], Step [600/704], Loss: 1.6325\n",
            "Epoch [1/7], Step [700/704], Loss: 1.4379\n",
            "Epoch [1/7] - Average Loss: 1.6248\n",
            "Validation Accuracy: 49.82%\n",
            "\n",
            "Epoch [2/7], Step [100/704], Loss: 1.1559\n",
            "Epoch [2/7], Step [200/704], Loss: 1.2775\n",
            "Epoch [2/7], Step [300/704], Loss: 0.9390\n",
            "Epoch [2/7], Step [400/704], Loss: 1.4637\n",
            "Epoch [2/7], Step [500/704], Loss: 1.0861\n",
            "Epoch [2/7], Step [600/704], Loss: 0.8214\n",
            "Epoch [2/7], Step [700/704], Loss: 0.7289\n",
            "Epoch [2/7] - Average Loss: 1.1465\n",
            "Validation Accuracy: 65.04%\n",
            "\n",
            "Epoch [3/7], Step [100/704], Loss: 0.8178\n",
            "Epoch [3/7], Step [200/704], Loss: 0.8814\n",
            "Epoch [3/7], Step [300/704], Loss: 0.9045\n",
            "Epoch [3/7], Step [400/704], Loss: 0.9674\n",
            "Epoch [3/7], Step [500/704], Loss: 0.8773\n",
            "Epoch [3/7], Step [600/704], Loss: 0.8862\n",
            "Epoch [3/7], Step [700/704], Loss: 0.8408\n",
            "Epoch [3/7] - Average Loss: 0.8551\n",
            "Validation Accuracy: 71.56%\n",
            "\n",
            "Epoch [4/7], Step [100/704], Loss: 0.7529\n",
            "Epoch [4/7], Step [200/704], Loss: 0.6648\n",
            "Epoch [4/7], Step [300/704], Loss: 0.6689\n",
            "Epoch [4/7], Step [400/704], Loss: 0.9397\n",
            "Epoch [4/7], Step [500/704], Loss: 0.5993\n",
            "Epoch [4/7], Step [600/704], Loss: 0.6331\n",
            "Epoch [4/7], Step [700/704], Loss: 0.4974\n",
            "Epoch [4/7] - Average Loss: 0.6884\n",
            "Validation Accuracy: 75.70%\n",
            "\n",
            "Epoch [5/7], Step [100/704], Loss: 0.8487\n",
            "Epoch [5/7], Step [200/704], Loss: 0.6920\n",
            "Epoch [5/7], Step [300/704], Loss: 0.5207\n",
            "Epoch [5/7], Step [400/704], Loss: 0.6777\n",
            "Epoch [5/7], Step [500/704], Loss: 0.5608\n",
            "Epoch [5/7], Step [600/704], Loss: 0.4970\n",
            "Epoch [5/7], Step [700/704], Loss: 0.6557\n",
            "Epoch [5/7] - Average Loss: 0.5817\n",
            "Validation Accuracy: 78.00%\n",
            "\n",
            "Epoch [6/7], Step [100/704], Loss: 0.5976\n",
            "Epoch [6/7], Step [200/704], Loss: 0.3543\n",
            "Epoch [6/7], Step [300/704], Loss: 0.7030\n",
            "Epoch [6/7], Step [400/704], Loss: 0.3951\n",
            "Epoch [6/7], Step [500/704], Loss: 0.7953\n",
            "Epoch [6/7], Step [600/704], Loss: 0.6375\n",
            "Epoch [6/7], Step [700/704], Loss: 0.3733\n",
            "Epoch [6/7] - Average Loss: 0.5126\n",
            "Validation Accuracy: 79.62%\n",
            "\n",
            "Epoch [7/7], Step [100/704], Loss: 0.5769\n",
            "Epoch [7/7], Step [200/704], Loss: 0.3943\n",
            "Epoch [7/7], Step [300/704], Loss: 0.3447\n",
            "Epoch [7/7], Step [400/704], Loss: 0.3901\n",
            "Epoch [7/7], Step [500/704], Loss: 0.6266\n",
            "Epoch [7/7], Step [600/704], Loss: 0.5235\n",
            "Epoch [7/7], Step [700/704], Loss: 0.5540\n",
            "Epoch [7/7] - Average Loss: 0.4563\n",
            "Validation Accuracy: 82.20%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SyvR0_JJ1vHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing"
      ],
      "metadata": {
        "id": "pLFWOxSqw1HH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final test evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_accuracy = 100 * correct / total\n",
        "    print(f'\\nFinal Test Accuracy: {test_accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Li6t8F0Bw5fP",
        "outputId": "cbb914e8-f5c6-45e5-a539-50cc3a6cb827"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Test Accuracy: 81.36%\n"
          ]
        }
      ]
    }
  ]
}