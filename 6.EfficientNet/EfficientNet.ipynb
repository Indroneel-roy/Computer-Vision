{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# EfficientNet Architecture Overview\n",
        "\n",
        "## Introduction\n",
        "\n",
        "EfficientNet is a family of **convolutional neural networks (CNNs)** designed to achieve **high accuracy with significantly fewer computational resources** compared to previous architectures.\n",
        "\n",
        "It was introduced by **Mingxing Tan and Quoc V. Le (Google Research)** in their 2019 paper:\n",
        "\n",
        "> **\"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\"**\n",
        "\n",
        "The key innovation of EfficientNet is **compound scaling**, which uniformly scales:\n",
        "- **Depth** (number of layers)\n",
        "- **Width** (number of channels)\n",
        "- **Input resolution**\n",
        "\n",
        "using a single scaling coefficient.\n",
        "\n",
        "---\n",
        "\n",
        "## Compound Scaling Idea\n",
        "\n",
        "Instead of arbitrarily increasing depth or width, EfficientNet uses a compound coefficient `φ`:\n",
        "\n",
        "- Depth ∝ α^φ  \n",
        "- Width ∝ β^φ  \n",
        "- Resolution ∝ γ^φ  \n",
        "\n",
        "subject to a computational constraint.\n",
        "\n",
        "This balanced scaling leads to **better accuracy per FLOP**.\n",
        "\n",
        "---\n",
        "\n",
        "## EfficientNet-B0 Architecture\n",
        "\n",
        "EfficientNet-B0 is the **baseline model** from which larger models (B1–B7) are derived.\n",
        "\n",
        "The architecture consists of three main parts:\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Stem\n",
        "\n",
        "The stem is the initial feature extraction layer.\n",
        "\n",
        "- Standard **3×3 convolution**\n",
        "- **32 filters**\n",
        "- **Stride = 2**\n",
        "- Followed by:\n",
        "  - Batch Normalization\n",
        "  - ReLU6 (or Swish in original paper)\n",
        "\n",
        "**Purpose:**\n",
        "- Extract low-level features (edges, textures)\n",
        "- Reduce spatial resolution\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Body (MBConv Blocks)\n",
        "\n",
        "The body is composed of multiple **MBConv (Mobile Inverted Bottleneck Convolution) blocks**.\n",
        "\n",
        "### MBConv Block Components\n",
        "\n",
        "Each MBConv block includes:\n",
        "\n",
        "1. **Expansion Layer**\n",
        "   - 1×1 convolution\n",
        "   - Expands channels by a factor (e.g., ×6)\n",
        "\n",
        "2. **Depthwise Convolution**\n",
        "   - 3×3 or 5×5 convolution\n",
        "   - Applied independently to each channel\n",
        "   - Reduces computation cost\n",
        "\n",
        "3. **Squeeze-and-Excitation (SE) Block**\n",
        "   - Channel-wise attention mechanism\n",
        "   - Learns which channels are important\n",
        "\n",
        "4. **Projection Layer**\n",
        "   - 1×1 convolution\n",
        "   - Reduces channels back to desired output\n",
        "   - Uses a **linear bottleneck** (no activation)\n",
        "\n",
        "5. **Residual Connection**\n",
        "   - Used when input and output shapes match\n",
        "\n",
        "---\n",
        "\n",
        "### MBConv Configuration Parameters\n",
        "\n",
        "Each MBConv stage is defined by:\n",
        "\n",
        "- **Expansion Ratio**\n",
        "- **Kernel Size**\n",
        "- **Stride**\n",
        "- **Number of Output Channels**\n",
        "- **Number of Repeats**\n",
        "- **SE Ratio**\n",
        "\n",
        "Different stages use different configurations to balance efficiency and accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Head\n",
        "\n",
        "The head converts extracted features into final predictions.\n",
        "\n",
        "- **1×1 convolution** to increase channels (usually to 1280)\n",
        "- **Global Average Pooling**\n",
        "- **Fully Connected (Dense) Layer**\n",
        "- **Softmax activation** for classification\n",
        "\n",
        "## Architecture Implementation\n",
        "![efficientNet](https://learnopencv.com/wp-content/uploads/2019/06/EfficientNet-B0-architecture-1024x511.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "qwDS2O_8BMP9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "4UI3n05QDm7I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zWAjD8yuApcs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from math import ceil"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define parameters"
      ],
      "metadata": {
        "id": "DPEqyG3nDpgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = [\n",
        "    # expand_ratio, channels, repeats, stride, kernel_size\n",
        "    [1, 16, 1, 1, 3],\n",
        "    [6, 24, 2, 2, 3],\n",
        "    [6, 40, 2, 2, 5],\n",
        "    [6, 80, 3, 2, 3],\n",
        "    [6, 112, 3, 1, 5],\n",
        "    [6, 192, 4, 2, 5],\n",
        "    [6, 320, 1, 1, 3],\n",
        "]\n",
        "\n",
        "phi_values = {\n",
        "    # tuple of: (phi_value, resolution, drop_rate)\n",
        "    \"b0\": (0, 224, 0.2),  # alpha, beta, gamma, depth = alpha ** phi\n",
        "    \"b1\": (0.5, 240, 0.2),\n",
        "    \"b2\": (1, 260, 0.3),\n",
        "    \"b3\": (2, 300, 0.3),\n",
        "    \"b4\": (3, 380, 0.4),\n",
        "    \"b5\": (4, 456, 0.4),\n",
        "    \"b6\": (5, 528, 0.5),\n",
        "    \"b7\": (6, 600, 0.5),\n",
        "}\n"
      ],
      "metadata": {
        "id": "gb5AH2M8D5jU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNNBlock"
      ],
      "metadata": {
        "id": "z0d0BmcMEIFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNBlock(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      in_channels,\n",
        "      out_channels,\n",
        "      kernel_size,\n",
        "      stride,\n",
        "      padding,\n",
        "      groups=1\n",
        "  ):\n",
        "    super(CNNBlock, self).__init__()\n",
        "    self.cnn = nn.Conv2d(\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel_size,\n",
        "        stride,\n",
        "        padding,\n",
        "        groups=groups, # if group=input channel, then a kernel is applied to one channel only\n",
        "        bias=False\n",
        "    )\n",
        "    self.bn = nn.BatchNorm2d(out_channels)\n",
        "    self.silu = nn.SiLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.silu(self.bn(self.cnn(x)))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UWvzJ6UrENXd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Squeeze & Excitation"
      ],
      "metadata": {
        "id": "x_uDFHX2FJh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SqueezeExcitation(nn.Module):\n",
        "  def __init__(self, in_channels, reduced_dim):\n",
        "    super(SqueezeExcitation, self).__init__()\n",
        "    self.se = nn.Sequential(\n",
        "        nn.AdaptiveAvgPool2d(1), # C x H x W -> C x 1 x 1\n",
        "        nn.Conv2d(in_channels, reduced_dim, 1),\n",
        "        nn.SiLU(),\n",
        "        nn.Conv2d(reduced_dim, in_channels, 1),\n",
        "        nn.Sigmoid(),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x * self.se(x)\n",
        ""
      ],
      "metadata": {
        "id": "BheINAvfHKIV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inverted residual block"
      ],
      "metadata": {
        "id": "4jcTxla7P-98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InvertedResidualBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel_size,\n",
        "        stride,\n",
        "        padding,\n",
        "        expand_ratio,\n",
        "        reduction=4,  # squeeze excitation\n",
        "        survival_prob=0.8,  # for stochastic depth\n",
        "    ):\n",
        "        super(InvertedResidualBlock, self).__init__()\n",
        "        self.survival_prob = 0.8\n",
        "        self.use_residual = in_channels == out_channels and stride == 1\n",
        "        hidden_dim = in_channels * expand_ratio\n",
        "        self.expand = in_channels != hidden_dim\n",
        "        reduced_dim = int(in_channels / reduction)\n",
        "\n",
        "        if self.expand:\n",
        "            self.expand_conv = CNNBlock(\n",
        "                in_channels,\n",
        "                hidden_dim,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1,\n",
        "            )\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            CNNBlock(\n",
        "                hidden_dim,\n",
        "                hidden_dim,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                groups=hidden_dim,\n",
        "            ),\n",
        "            SqueezeExcitation(hidden_dim, reduced_dim),\n",
        "            nn.Conv2d(hidden_dim, out_channels, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "        )\n",
        "\n",
        "    def stochastic_depth(self, x):\n",
        "        if not self.training:\n",
        "            return x\n",
        "\n",
        "        binary_tensor = (\n",
        "            torch.rand(x.shape[0], 1, 1, 1, device=x.device) < self.survival_prob\n",
        "        )\n",
        "        return torch.div(x, self.survival_prob) * binary_tensor\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.expand_conv(inputs) if self.expand else inputs\n",
        "\n",
        "        if self.use_residual:\n",
        "            return self.stochastic_depth(self.conv(x)) + inputs\n",
        "        else:\n",
        "            return self.conv(x)\n"
      ],
      "metadata": {
        "id": "KT4hJt18QJ1N"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EfficientNet"
      ],
      "metadata": {
        "id": "kr4VKJsDRGSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EfficientNet(nn.Module):\n",
        "    def __init__(self, version, num_classes):\n",
        "        super(EfficientNet, self).__init__()\n",
        "        width_factor, depth_factor, dropout_rate = self.calculate_factors(version)\n",
        "        last_channels = ceil(1280 * width_factor)\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.features = self.create_features(width_factor, depth_factor, last_channels)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(last_channels, num_classes),\n",
        "        )\n",
        "\n",
        "    def calculate_factors(self, version, alpha=1.2, beta=1.1):\n",
        "        phi, res, drop_rate = phi_values[version]\n",
        "        depth_factor = alpha**phi\n",
        "        width_factor = beta**phi\n",
        "        return width_factor, depth_factor, drop_rate\n",
        "\n",
        "    def create_features(self, width_factor, depth_factor, last_channels):\n",
        "        channels = int(32 * width_factor)\n",
        "        features = [CNNBlock(3, channels, 3, stride=2, padding=1)]\n",
        "        in_channels = channels\n",
        "\n",
        "        for expand_ratio, channels, repeats, stride, kernel_size in base_model:\n",
        "            out_channels = 4 * ceil(int(channels * width_factor) / 4)\n",
        "            layers_repeats = ceil(repeats * depth_factor)\n",
        "\n",
        "            for layer in range(layers_repeats):\n",
        "                features.append(\n",
        "                    InvertedResidualBlock(\n",
        "                        in_channels,\n",
        "                        out_channels,\n",
        "                        expand_ratio=expand_ratio,\n",
        "                        stride=stride if layer == 0 else 1,\n",
        "                        kernel_size=kernel_size,\n",
        "                        padding=kernel_size // 2,  # if k=1:pad=0, k=3:pad=1, k=5:pad=2\n",
        "                    )\n",
        "                )\n",
        "                in_channels = out_channels\n",
        "\n",
        "        features.append(\n",
        "            CNNBlock(in_channels, last_channels, kernel_size=1, stride=1, padding=0)\n",
        "        )\n",
        "\n",
        "        return nn.Sequential(*features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.features(x))\n",
        "        return self.classifier(x.view(x.shape[0], -1))\n"
      ],
      "metadata": {
        "id": "-2lBQiapROZP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset and Preprocessing"
      ],
      "metadata": {
        "id": "7d36fyF9RPnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ],
      "metadata": {
        "id": "FjqzBnUySy0G"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_valid_loader(data_dir, batch_size, augment, random_seed, valid_size=0.1, shuffle=True):\n",
        "    \"\"\"Get training and validation data loaders for CIFAR-10\"\"\"\n",
        "\n",
        "    # CIFAR-10 normalization\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.4914, 0.4822, 0.4465],\n",
        "        std=[0.2023, 0.1994, 0.2010]\n",
        "    )\n",
        "\n",
        "    # Common transforms\n",
        "    common_transform = [\n",
        "        transforms.Resize((224, 224)),  # Resize to standard ResNet input\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ]\n",
        "\n",
        "    # Training transforms with optional augmentation\n",
        "    if augment:\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomCrop(224, padding=4),\n",
        "            transforms.ToTensor(),\n",
        "            normalize\n",
        "        ])\n",
        "    else:\n",
        "        train_transform = transforms.Compose(common_transform)\n",
        "\n",
        "    valid_transform = transforms.Compose(common_transform)\n",
        "\n",
        "    # Load datasets\n",
        "    train_dataset = datasets.CIFAR10(\n",
        "        root=data_dir, train=True, download=True, transform=train_transform\n",
        "    )\n",
        "    valid_dataset = datasets.CIFAR10(\n",
        "        root=data_dir, train=True, download=True, transform=valid_transform\n",
        "    )\n",
        "\n",
        "    # Create train/valid split\n",
        "    num_train = len(train_dataset)\n",
        "    indices = list(range(num_train))\n",
        "    split = int(np.floor(valid_size * num_train))\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.seed(random_seed)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    train_idx, valid_idx = indices[split:], indices[:split]\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=batch_size, sampler=train_sampler\n",
        "    )\n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset, batch_size=batch_size, sampler=valid_sampler\n",
        "    )\n",
        "\n",
        "    return train_loader, valid_loader"
      ],
      "metadata": {
        "id": "7t_xBM70Ssqv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_test_loader(data_dir, batch_size, shuffle=True):\n",
        "    \"\"\"Get test data loader for CIFAR-10\"\"\"\n",
        "\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.4914, 0.4822, 0.4465],\n",
        "        std=[0.2023, 0.1994, 0.2010]\n",
        "    )\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "\n",
        "    dataset = datasets.CIFAR10(\n",
        "        root=data_dir, train=False, download=True, transform=transform\n",
        "    )\n",
        "\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=shuffle\n",
        "    )\n",
        "\n",
        "    return data_loader"
      ],
      "metadata": {
        "id": "qpF1baQ4S4KX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "data_dir = './data'\n",
        "num_classes = 10  # CIFAR-10 has 10 classes\n",
        "num_epochs = 7\n",
        "batch_size = 64\n",
        "learning_rate = 0.01\n",
        "version = \"b0\"\n",
        "phi, res, drop_rate = phi_values[version]\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load data\n",
        "train_loader, valid_loader = get_train_valid_loader(\n",
        "    data_dir=data_dir,\n",
        "    batch_size=batch_size,\n",
        "    augment=True,  # Enable data augmentation\n",
        "    random_seed=1\n",
        ")\n",
        "\n",
        "test_loader = get_test_loader(data_dir=data_dir, batch_size=batch_size)\n",
        "\n",
        "# Create model - FIXED: Using correct num_classes\n",
        "model = EfficientNet(version, num_classes=num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=learning_rate,\n",
        "    weight_decay=0.0001,\n",
        "    momentum=0.9\n",
        ")\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ulrQxkxS902",
        "outputId": "f660b9ad-02f1-41e2-fd59-5066cc8f79a1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 35.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "qnvWLRmmTnVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "total_step = len(train_loader)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Print progress every 100 steps\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Epoch summary\n",
        "    avg_loss = running_loss / total_step\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}] - Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in valid_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f'Validation Accuracy: {accuracy:.2f}%\\n')\n",
        "\n",
        "    # Step the scheduler\n",
        "    scheduler.step()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bx_G4BOnTpMW",
        "outputId": "97a79a34-9194-45eb-ff61-4ed7af78402a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/7], Step [100/704], Loss: 2.1491\n",
            "Epoch [1/7], Step [200/704], Loss: 1.9304\n",
            "Epoch [1/7], Step [300/704], Loss: 1.6972\n",
            "Epoch [1/7], Step [400/704], Loss: 1.4701\n",
            "Epoch [1/7], Step [500/704], Loss: 1.3958\n",
            "Epoch [1/7], Step [600/704], Loss: 1.2107\n",
            "Epoch [1/7], Step [700/704], Loss: 1.3300\n",
            "Epoch [1/7] - Average Loss: 1.6162\n",
            "Validation Accuracy: 57.76%\n",
            "\n",
            "Epoch [2/7], Step [100/704], Loss: 1.1870\n",
            "Epoch [2/7], Step [200/704], Loss: 1.1734\n",
            "Epoch [2/7], Step [300/704], Loss: 1.1404\n",
            "Epoch [2/7], Step [400/704], Loss: 1.3840\n",
            "Epoch [2/7], Step [500/704], Loss: 0.8958\n",
            "Epoch [2/7], Step [600/704], Loss: 0.9078\n",
            "Epoch [2/7], Step [700/704], Loss: 0.7448\n",
            "Epoch [2/7] - Average Loss: 1.0084\n",
            "Validation Accuracy: 70.72%\n",
            "\n",
            "Epoch [3/7], Step [100/704], Loss: 0.6678\n",
            "Epoch [3/7], Step [200/704], Loss: 0.9212\n",
            "Epoch [3/7], Step [300/704], Loss: 0.5874\n",
            "Epoch [3/7], Step [400/704], Loss: 0.8491\n",
            "Epoch [3/7], Step [500/704], Loss: 0.5146\n",
            "Epoch [3/7], Step [600/704], Loss: 0.6741\n",
            "Epoch [3/7], Step [700/704], Loss: 0.5528\n",
            "Epoch [3/7] - Average Loss: 0.7410\n",
            "Validation Accuracy: 78.40%\n",
            "\n",
            "Epoch [4/7], Step [100/704], Loss: 0.7377\n",
            "Epoch [4/7], Step [200/704], Loss: 0.6494\n",
            "Epoch [4/7], Step [300/704], Loss: 0.6902\n",
            "Epoch [4/7], Step [400/704], Loss: 0.6912\n",
            "Epoch [4/7], Step [500/704], Loss: 0.5061\n",
            "Epoch [4/7], Step [600/704], Loss: 0.4754\n",
            "Epoch [4/7], Step [700/704], Loss: 0.5880\n",
            "Epoch [4/7] - Average Loss: 0.6018\n",
            "Validation Accuracy: 81.06%\n",
            "\n",
            "Epoch [5/7], Step [100/704], Loss: 0.3992\n",
            "Epoch [5/7], Step [200/704], Loss: 0.4880\n",
            "Epoch [5/7], Step [300/704], Loss: 0.8966\n",
            "Epoch [5/7], Step [400/704], Loss: 0.5511\n",
            "Epoch [5/7], Step [500/704], Loss: 0.4378\n",
            "Epoch [5/7], Step [600/704], Loss: 0.5204\n",
            "Epoch [5/7], Step [700/704], Loss: 0.7608\n",
            "Epoch [5/7] - Average Loss: 0.5131\n",
            "Validation Accuracy: 83.76%\n",
            "\n",
            "Epoch [6/7], Step [100/704], Loss: 0.5754\n",
            "Epoch [6/7], Step [200/704], Loss: 0.6535\n",
            "Epoch [6/7], Step [300/704], Loss: 0.8347\n",
            "Epoch [6/7], Step [400/704], Loss: 0.3119\n",
            "Epoch [6/7], Step [500/704], Loss: 0.6649\n",
            "Epoch [6/7], Step [600/704], Loss: 0.4188\n",
            "Epoch [6/7], Step [700/704], Loss: 0.2854\n",
            "Epoch [6/7] - Average Loss: 0.4496\n",
            "Validation Accuracy: 84.30%\n",
            "\n",
            "Epoch [7/7], Step [100/704], Loss: 0.3284\n",
            "Epoch [7/7], Step [200/704], Loss: 0.2480\n",
            "Epoch [7/7], Step [300/704], Loss: 0.2268\n",
            "Epoch [7/7], Step [400/704], Loss: 0.4698\n",
            "Epoch [7/7], Step [500/704], Loss: 0.4230\n",
            "Epoch [7/7], Step [600/704], Loss: 0.3691\n",
            "Epoch [7/7], Step [700/704], Loss: 0.2601\n",
            "Epoch [7/7] - Average Loss: 0.3893\n",
            "Validation Accuracy: 85.30%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "eYZTVFHnTvxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final test evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_accuracy = 100 * correct / total\n",
        "    print(f'\\nFinal Test Accuracy: {test_accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDs7ML-zT1eD",
        "outputId": "39443f2f-39b3-43be-afee-2d92bb9bc2f4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Test Accuracy: 85.21%\n"
          ]
        }
      ]
    }
  ]
}